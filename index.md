# Hi, I'm Yurii — Data Engineer

I build reliable data pipelines with **Spark (PySpark), Airflow, Kafka, and GCP**.

## Portfolio

### 1) Batch ETL: CSV → PySpark → Parquet
**What it does:** aggregates a small dataset and writes Parquet.  
**Stack:** PySpark 3.5, Python 3.11.  
**Code:** [de-batch-etl](https://github.com/YuriiKiktenko/de-batch-etl)

> Next: Airflow DAG (daily), Kafka → Spark Structured Streaming → Data Lake.
